{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3c692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os, re, glob\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa4a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== MFCC sekvens (bevarer tidsaksen) ======\n",
    "\n",
    "\n",
    "# --- Vinduer: 400 ms med 50% overlap ---\n",
    "def slice_to_windows(x, sr, win_ms=400, hop_ms=200):\n",
    "    win = int(round(sr * win_ms/1000))\n",
    "    hop = int(round(sr * hop_ms/1000))\n",
    "    if len(x) < win:\n",
    "        x = np.pad(x, (0, win - len(x)))\n",
    "    starts = np.arange(0, len(x) - win + 1, hop, dtype=int)\n",
    "    windows = [x[s:s+win] for s in starts]\n",
    "    return np.stack(windows) if windows else np.empty((0, win), dtype=np.float32)\n",
    "\n",
    "# --- MFCC-sekvens: EI-match (ingen deltas) ---\n",
    "import numpy as np, librosa\n",
    "\n",
    "def mfcc_sequence_EI(signal, sample_rate,\n",
    "                     n_mfcc=13, n_mels=32,\n",
    "                     win_length_ms=25, hop_length_ms=20,\n",
    "                     fmin=80, fmax=None,\n",
    "                     n_fft=512, pre_emph=0.98,\n",
    "                     center=False, add_deltas=False):\n",
    "    x = np.asarray(signal, dtype=np.float32)\n",
    "    if x.ndim == 2:\n",
    "        x = x.mean(axis=1)\n",
    "\n",
    "    # pre-emphasis 0.98\n",
    "    x = np.append(x[0], x[1:] - pre_emph * x[:-1])\n",
    "\n",
    "    n_fft = int(n_fft)\n",
    "    hop   = int(round(sample_rate * hop_length_ms / 1000))\n",
    "    win   = int(round(sample_rate * win_length_ms / 1000))\n",
    "    if fmax is None:\n",
    "        fmax = sample_rate / 2\n",
    "\n",
    "    # mel power → dB (ref=1.0 for absolut skala)\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=x, sr=sample_rate, n_fft=n_fft, hop_length=hop, win_length=win,\n",
    "        window=\"hann\", center=center, power=2.0,\n",
    "        n_mels=n_mels, fmin=fmin, fmax=fmax, htk=True\n",
    "    )\n",
    "    S_db = librosa.power_to_db(S, ref=1.0, top_db=80.0)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(S=S_db, sr=sample_rate, n_mfcc=n_mfcc)  # (n_mfcc, T)\n",
    "\n",
    "    feats = [mfcc]\n",
    "    if add_deltas:\n",
    "        d1 = librosa.feature.delta(mfcc, order=1, width=9, mode=\"nearest\")\n",
    "        d2 = librosa.feature.delta(mfcc, order=2, width=9, mode=\"nearest\")\n",
    "        feats += [d1, d2]\n",
    "    X = np.concatenate(feats, axis=0).T  # (T, F)\n",
    "\n",
    "    # \"Normalization window size = 151\": T<<151 → per-vindue CMVN ~ global.\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True) + 1e-6\n",
    "    X = (X - mu) / sd\n",
    "\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "# ====== Læs én WAV → (X, y, car, files) ======\n",
    "def process_wav_mfcc_conv1d(file_path, label_int, car_id,\n",
    "                            sr_expected=None,\n",
    "                            win_ms=400, hop_ms=200,\n",
    "                            n_mfcc=13, n_mels=32,\n",
    "                            win_length_ms=25, hop_length_ms=20):\n",
    "    sr, data = wavfile.read(file_path)\n",
    "    x = np.asarray(data, dtype=np.float32)\n",
    "    if x.ndim == 2:\n",
    "        x = x.mean(axis=1)\n",
    "    mx = np.max(np.abs(x))\n",
    "    if mx > 0: x = x / mx\n",
    "    if sr_expected is not None and sr != sr_expected:\n",
    "        # (tilpas evt. med librosa.resample)\n",
    "        raise ValueError(f\"Sample rate mismatch: {sr} != {sr_expected}\")\n",
    "\n",
    "    segments = slice_to_windows(x, sr, win_ms, hop_ms)  # (N, samples)\n",
    "    X_list = []\n",
    "    for seg in segments:\n",
    "        X_seq = mfcc_sequence_EI(seg, sample_rate=sr,\n",
    "                              n_mfcc=n_mfcc, n_mels=n_mels,\n",
    "                              win_length_ms=win_length_ms, hop_length_ms=hop_length_ms,\n",
    "                              add_deltas=False, center=False)  # (T,F)\n",
    "        X_list.append(X_seq)\n",
    "\n",
    "    if not X_list:\n",
    "        return (np.empty((0,1,1), np.float32),\n",
    "                np.empty((0,), np.int32),\n",
    "                np.empty((0,), object),\n",
    "                np.empty((0,), object))\n",
    "\n",
    "    X = np.stack(X_list)                          # (N, T, F)\n",
    "    y = np.full((X.shape[0],), label_int, np.int32)\n",
    "    car = np.full((X.shape[0],), car_id, object)\n",
    "    files = np.array([os.path.basename(file_path)] * X.shape[0], object)\n",
    "    return X, y, car, files\n",
    "\n",
    "# ====== Scan mapper og byg hele datasættet ======\n",
    "def build_dataset_conv1d(root_dir, sr_expected=None):\n",
    "    # Find NORMAL/FAULTY\n",
    "    normal_paths = glob.glob(os.path.join(root_dir, \"NORMAL\", \"**\", \"*.wav\"), recursive=True)\n",
    "    faulty_paths = glob.glob(os.path.join(root_dir, \"FAULTY\", \"**\", \"*.wav\"), recursive=True)\n",
    "\n",
    "    label_map = {\"NORMAL\": 0, \"FAULTY\": 1}\n",
    "\n",
    "    # Hjælper: udtræk car_id fra filnavn (tilpas regex til dit mønster)\n",
    "    car_re = re.compile(r\"(car\\d+)\", re.IGNORECASE)\n",
    "\n",
    "    X_list, y_list, g_list, f_list = [], [], [], []\n",
    "\n",
    "    for label_name, paths in [(\"NORMAL\", normal_paths), (\"FAULTY\", faulty_paths)]:\n",
    "        for p in paths:\n",
    "            m = car_re.search(os.path.basename(p))\n",
    "            car_id = (m.group(1) if m else \"UNKNOWN\").upper()\n",
    "            Xi, yi, gi, fi = process_wav_mfcc_conv1d(\n",
    "                p, label_map[label_name], car_id,\n",
    "                sr_expected=sr_expected,\n",
    "                win_ms=400, hop_ms=200,\n",
    "                n_mfcc=13, n_mels=32,\n",
    "                win_length_ms=25, hop_length_ms=20\n",
    "            )\n",
    "            if Xi.shape[0] == 0:\n",
    "                continue\n",
    "            X_list.append(Xi)\n",
    "            y_list.append(yi)\n",
    "            g_list.append(gi)\n",
    "            f_list.append(fi)\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0) if X_list else np.empty((0,1,1), np.float32)\n",
    "    y = np.concatenate(y_list, axis=0) if y_list else np.empty((0,), np.int32)\n",
    "    groups = np.concatenate(g_list, axis=0) if g_list else np.empty((0,), object)\n",
    "    files = np.concatenate(f_list, axis=0) if f_list else np.empty((0,), object)\n",
    "    return X, y, groups, files, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1578150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (19665, 19, 13)\n",
      "y: (19665,)\n",
      "unique cars: ['CAR01' 'CAR02' 'CAR03' 'CAR04']\n"
     ]
    }
   ],
   "source": [
    "root_dir = r\"C:\\Users\\nusse\\Desktop\\EH5TinyML\\soundovertcp\\Server\\out\"\n",
    "X, y, groups, files, label_map = build_dataset_conv1d(root_dir, sr_expected=None)\n",
    "\n",
    "print(\"X:\", X.shape)     # (N, T, F) \n",
    "print(\"y:\", y.shape)\n",
    "print(\"unique cars:\", pd.unique(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66da9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def stratified_per_car_split(groups, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Returnér tr_idx, te_idx sådan at:\n",
    "      - HVER bil (group) er til stede i både train og test\n",
    "      - FOR HVER bil er test-delen stratificeret på y (NORMAL/FAULTY)\n",
    "    \"\"\"\n",
    "    groups = np.asarray(groups)\n",
    "    y = np.asarray(y)\n",
    "    all_idx = np.arange(len(y))\n",
    "\n",
    "    te_mask = np.zeros(len(y), dtype=bool)\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    for car in np.unique(groups):\n",
    "        idx_car = all_idx[groups == car]\n",
    "        y_car = y[idx_car]\n",
    "        # stratificeret split INDEN i denne bil\n",
    "        _, te_idx_car = train_test_split(\n",
    "            idx_car, test_size=test_size, stratify=y_car, random_state=rng.randint(0, 10**9)\n",
    "        )\n",
    "        te_mask[te_idx_car] = True\n",
    "\n",
    "    te_idx = np.where(te_mask)[0]\n",
    "    tr_idx = np.where(~te_mask)[0]\n",
    "    return tr_idx, te_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf04e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cars: ['CAR01' 'CAR02' 'CAR03' 'CAR04']\n",
      "Test  cars: ['CAR01' 'CAR02' 'CAR03' 'CAR04']\n",
      "TEST per car & label:\n",
      "y        0    1\n",
      "car            \n",
      "CAR01  477  479\n",
      "CAR02  480  479\n",
      "CAR03  477  587\n",
      "CAR04  477  479\n"
     ]
    }
   ],
   "source": [
    "# lav split\n",
    "tr_idx, te_idx = stratified_per_car_split(groups, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# lav dine datasæt\n",
    "Xtr, Xte = X[tr_idx], X[te_idx]\n",
    "ytr, yte = y[tr_idx], y[te_idx]\n",
    "groups_tr, groups_te = groups[tr_idx], groups[te_idx]\n",
    "\n",
    "# sanity-check: hver bil findes i begge sæt, og test har begge labels pr. bil\n",
    "import pandas as pd\n",
    "print(\"Train cars:\", np.unique(groups_tr))\n",
    "print(\"Test  cars:\", np.unique(groups_te))\n",
    "\n",
    "print(\"TEST per car & label:\")\n",
    "df_te = pd.DataFrame({'car': groups_te, 'y': yte})\n",
    "print(df_te.groupby(['car','y']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf79eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (12584, 19, 13) Val: (3146, 19, 13) Test: (3935, 19, 13)\n"
     ]
    }
   ],
   "source": [
    "# Split train -> train + val, stadig stratificeret pr. bil\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr2_idx, val_idx = train_test_split(\n",
    "    np.arange(len(Xtr)),\n",
    "    test_size=0.2,\n",
    "    stratify=ytr,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "X_tr, X_val = Xtr[tr2_idx], Xtr[val_idx]\n",
    "y_tr, y_val = ytr[tr2_idx], ytr[val_idx]\n",
    "groups_tr2, groups_val = groups_tr[tr2_idx], groups_tr[val_idx]\n",
    "\n",
    "print(\"Train:\", X_tr.shape, \"Val:\", X_val.shape, \"Test:\", Xte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b48e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nusse\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 - 2s - 6ms/step - accuracy: 0.7923 - loss: 0.4008 - val_accuracy: 0.9282 - val_loss: 0.1766 - learning_rate: 0.0050\n",
      "Epoch 2/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9105 - loss: 0.2114 - val_accuracy: 0.9720 - val_loss: 0.0761 - learning_rate: 0.0050\n",
      "Epoch 3/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9336 - loss: 0.1609 - val_accuracy: 0.9739 - val_loss: 0.0795 - learning_rate: 0.0050\n",
      "Epoch 4/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9429 - loss: 0.1499 - val_accuracy: 0.9812 - val_loss: 0.0569 - learning_rate: 0.0050\n",
      "Epoch 5/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9459 - loss: 0.1331 - val_accuracy: 0.9800 - val_loss: 0.0605 - learning_rate: 0.0050\n",
      "Epoch 6/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9527 - loss: 0.1220 - val_accuracy: 0.9882 - val_loss: 0.0417 - learning_rate: 0.0050\n",
      "Epoch 7/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9509 - loss: 0.1229 - val_accuracy: 0.9851 - val_loss: 0.0462 - learning_rate: 0.0050\n",
      "Epoch 8/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9558 - loss: 0.1181 - val_accuracy: 0.9882 - val_loss: 0.0360 - learning_rate: 0.0050\n",
      "Epoch 9/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9561 - loss: 0.1104 - val_accuracy: 0.9790 - val_loss: 0.0703 - learning_rate: 0.0050\n",
      "Epoch 10/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9584 - loss: 0.1060 - val_accuracy: 0.9847 - val_loss: 0.0437 - learning_rate: 0.0050\n",
      "Epoch 11/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9541 - loss: 0.1176 - val_accuracy: 0.9873 - val_loss: 0.0354 - learning_rate: 0.0050\n",
      "Epoch 12/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9606 - loss: 0.1017 - val_accuracy: 0.9841 - val_loss: 0.0417 - learning_rate: 0.0050\n",
      "Epoch 13/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9620 - loss: 0.1045 - val_accuracy: 0.9889 - val_loss: 0.0357 - learning_rate: 0.0050\n",
      "Epoch 14/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9599 - loss: 0.1025 - val_accuracy: 0.9901 - val_loss: 0.0291 - learning_rate: 0.0050\n",
      "Epoch 15/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9599 - loss: 0.1058 - val_accuracy: 0.9905 - val_loss: 0.0314 - learning_rate: 0.0050\n",
      "Epoch 16/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9586 - loss: 0.1038 - val_accuracy: 0.9905 - val_loss: 0.0275 - learning_rate: 0.0050\n",
      "Epoch 17/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9619 - loss: 0.0962 - val_accuracy: 0.9911 - val_loss: 0.0255 - learning_rate: 0.0050\n",
      "Epoch 18/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9627 - loss: 0.0953 - val_accuracy: 0.9873 - val_loss: 0.0393 - learning_rate: 0.0050\n",
      "Epoch 19/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9603 - loss: 0.0979 - val_accuracy: 0.9927 - val_loss: 0.0275 - learning_rate: 0.0050\n",
      "Epoch 20/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9632 - loss: 0.1004 - val_accuracy: 0.9832 - val_loss: 0.0503 - learning_rate: 0.0050\n",
      "Epoch 21/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9601 - loss: 0.1002 - val_accuracy: 0.9901 - val_loss: 0.0297 - learning_rate: 0.0050\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9613 - loss: 0.1006 - val_accuracy: 0.9898 - val_loss: 0.0301 - learning_rate: 0.0050\n",
      "Epoch 23/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9661 - loss: 0.0856 - val_accuracy: 0.9924 - val_loss: 0.0240 - learning_rate: 0.0025\n",
      "Epoch 24/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9697 - loss: 0.0776 - val_accuracy: 0.9924 - val_loss: 0.0289 - learning_rate: 0.0025\n",
      "Epoch 25/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9708 - loss: 0.0762 - val_accuracy: 0.9927 - val_loss: 0.0254 - learning_rate: 0.0025\n",
      "Epoch 26/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9730 - loss: 0.0680 - val_accuracy: 0.9943 - val_loss: 0.0201 - learning_rate: 0.0025\n",
      "Epoch 27/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9714 - loss: 0.0734 - val_accuracy: 0.9946 - val_loss: 0.0175 - learning_rate: 0.0025\n",
      "Epoch 28/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9731 - loss: 0.0697 - val_accuracy: 0.9927 - val_loss: 0.0269 - learning_rate: 0.0025\n",
      "Epoch 29/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9721 - loss: 0.0740 - val_accuracy: 0.9940 - val_loss: 0.0211 - learning_rate: 0.0025\n",
      "Epoch 30/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9713 - loss: 0.0733 - val_accuracy: 0.9940 - val_loss: 0.0188 - learning_rate: 0.0025\n",
      "Epoch 31/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9700 - loss: 0.0767 - val_accuracy: 0.9892 - val_loss: 0.0373 - learning_rate: 0.0025\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9747 - loss: 0.0699 - val_accuracy: 0.9921 - val_loss: 0.0274 - learning_rate: 0.0025\n",
      "Epoch 33/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9746 - loss: 0.0694 - val_accuracy: 0.9927 - val_loss: 0.0213 - learning_rate: 0.0012\n",
      "Epoch 34/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9736 - loss: 0.0665 - val_accuracy: 0.9936 - val_loss: 0.0204 - learning_rate: 0.0012\n",
      "Epoch 35/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9770 - loss: 0.0618 - val_accuracy: 0.9933 - val_loss: 0.0223 - learning_rate: 0.0012\n",
      "Epoch 36/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9765 - loss: 0.0595 - val_accuracy: 0.9949 - val_loss: 0.0161 - learning_rate: 0.0012\n",
      "Epoch 37/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9767 - loss: 0.0602 - val_accuracy: 0.9940 - val_loss: 0.0190 - learning_rate: 0.0012\n",
      "Epoch 38/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9759 - loss: 0.0677 - val_accuracy: 0.9927 - val_loss: 0.0214 - learning_rate: 0.0012\n",
      "Epoch 39/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9751 - loss: 0.0621 - val_accuracy: 0.9949 - val_loss: 0.0181 - learning_rate: 0.0012\n",
      "Epoch 40/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9793 - loss: 0.0581 - val_accuracy: 0.9921 - val_loss: 0.0238 - learning_rate: 0.0012\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9767 - loss: 0.0606 - val_accuracy: 0.9936 - val_loss: 0.0180 - learning_rate: 0.0012\n",
      "Epoch 42/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9808 - loss: 0.0519 - val_accuracy: 0.9930 - val_loss: 0.0183 - learning_rate: 6.2500e-04\n",
      "Epoch 43/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9779 - loss: 0.0596 - val_accuracy: 0.9940 - val_loss: 0.0177 - learning_rate: 6.2500e-04\n",
      "Epoch 44/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9810 - loss: 0.0494 - val_accuracy: 0.9943 - val_loss: 0.0179 - learning_rate: 6.2500e-04\n",
      "Epoch 45/100\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9819 - loss: 0.0490 - val_accuracy: 0.9940 - val_loss: 0.0178 - learning_rate: 6.2500e-04\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "394/394 - 1s - 3ms/step - accuracy: 0.9794 - loss: 0.0520 - val_accuracy: 0.9943 - val_loss: 0.0194 - learning_rate: 6.2500e-04\n",
      "VAL acc: 0.994914174079895\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ====== 0) Sikr korrekt format og one-hot ======\n",
    "assert X_tr.ndim == 3 and X_tr.shape[2] == 13, f\"forvent (N,T,13), fik {X_tr.shape}\"\n",
    "assert X_val.ndim == 3 and X_val.shape[2] == 13\n",
    "num_classes = int(np.unique(y_tr).size)\n",
    "assert num_classes == 2, \"den her skabelon er sat op til NORMAL/FAULTY (2 klasser)\"\n",
    "\n",
    "# one-hot labels (EI bruger categorical_crossentropy + softmax)\n",
    "y_tr_oh  = tf.one_hot(y_tr.astype(np.int32), depth=num_classes)\n",
    "y_val_oh = tf.one_hot(y_val.astype(np.int32), depth=num_classes)\n",
    "\n",
    "# Flatten to (input_length,) for EI-style InputLayer + Reshape til (T,13)\n",
    "T, F = X_tr.shape[1], X_tr.shape[2]    # fx (19, 13)\n",
    "input_length = T * F                   # fx 247\n",
    "X_tr_flat = X_tr.reshape((-1, input_length)).astype(np.float32)\n",
    "X_val_flat = X_val.reshape((-1, input_length)).astype(np.float32)\n",
    "\n",
    "# ====== 1) SpecAugment-lignende mapper (time/freq mask) ======\n",
    "# (meget letvægts version; samme idé som EI’s SpecAugment-parametre)\n",
    "def specaugment_mapper(mF=1, Fmax=4, mT=1, Tmax=1):\n",
    "    # forventer (input_length,) -> vi reshaper til (T,13) inde i mappet\n",
    "    def _map(x, y):\n",
    "        x = tf.reshape(x, (T, F))  # (T,13)\n",
    "\n",
    "        # freq masks\n",
    "        for _ in range(mF):\n",
    "            f = tf.random.uniform([], minval=0, maxval=Fmax+1, dtype=tf.int32)\n",
    "            f0 = tf.random.uniform([], minval=0, maxval=F - f + 1, dtype=tf.int32)\n",
    "            mask = tf.concat([\n",
    "                tf.ones((T, f0), dtype=x.dtype),\n",
    "                tf.zeros((T, f), dtype=x.dtype),\n",
    "                tf.ones((T, F - f0 - f), dtype=x.dtype)\n",
    "            ], axis=1)\n",
    "            x = x * mask\n",
    "\n",
    "        # time masks\n",
    "        for _ in range(mT):\n",
    "            t = tf.random.uniform([], minval=0, maxval=Tmax+1, dtype=tf.int32)\n",
    "            t0 = tf.random.uniform([], minval=0, maxval=T - t + 1, dtype=tf.int32)\n",
    "            mask = tf.concat([\n",
    "                tf.ones((t0, F), dtype=x.dtype),\n",
    "                tf.zeros((t, F), dtype=x.dtype),\n",
    "                tf.ones((T - t0 - t, F), dtype=x.dtype)\n",
    "            ], axis=0)\n",
    "            x = x * mask\n",
    "\n",
    "        # tilbage til (input_length,)\n",
    "        x = tf.reshape(x, (input_length,))\n",
    "        return x, y\n",
    "    return _map\n",
    "\n",
    "# ====== 2) tf.data pipelines ======\n",
    "BATCH_SIZE = 32\n",
    "ENSURE_DETERMINISM = False\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCHS = 100\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_tr_flat, y_tr_oh))\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((X_val_flat, y_val_oh))\n",
    "\n",
    "if not ENSURE_DETERMINISM:\n",
    "    train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE*4, reshuffle_each_iteration=True)\n",
    "\n",
    "# GaussianNoise ligger i modellen; her laver vi SpecAugment på træning\n",
    "sa = specaugment_mapper(mF=1, Fmax=4, mT=1, Tmax=1)  # svarer ca. til EI’s params i dit snippet\n",
    "train_ds = train_ds.map(sa, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = train_ds.batch(BATCH_SIZE, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = val_ds.batch(BATCH_SIZE, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ====== 3) Model (EI-style) ======\n",
    "# Bemærk: vi følger dit EI-snippet tæt: GaussianNoise -> Reshape -> Conv1D×3 -> MaxPool -> Flatten -> Dropout -> Dense softmax\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(input_length,)),\n",
    "    layers.GaussianNoise(stddev=0.2),                     # EI: GaussianNoise på flattened input\n",
    "    layers.Reshape((T, F)),                                # -> (T,13)\n",
    "    layers.Conv1D(16, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "    layers.Conv1D(32, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "    layers.Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, name='y_pred', activation='softmax'),\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# (valgfri) callbacks ala EI\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                         patience=5, min_lr=1e-5, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                     restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# ====== 4) Train & eval ======\n",
    "history = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, verbose=2, callbacks=callbacks)\n",
    "val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
    "print(\"VAL acc:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808a4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed\n",
      "Gathering Weights\n",
      "Writing layer  keras_tensor_82\n",
      "Writing layer  keras_tensor_83\n",
      "Writing layer  keras_tensor_84\n",
      "Writing layer  keras_tensor_85\n",
      "Writing layer  keras_tensor_86\n",
      "Writing layer  keras_tensor_87\n",
      "Writing layer  keras_tensor_88\n",
      "Writing layer  keras_tensor_89\n",
      "Writing layer  keras_tensor_90\n",
      "Writing layer  keras_tensor_91\n",
      "astyle not found, mymodel.h and mymodel.c will not be auto-formatted\n",
      "Writing tests\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nusse\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "astyle not found, mymodel_test_suite.c will not be auto-formatted\n",
      "Done \n",
      "C code is in 'mymodel.c' with header file 'mymodel.h' \n",
      "Tests are in 'mymodel_test_suite.c' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras2c import k2c\n",
    "model_inf = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(input_length,)),\n",
    "    layers.Reshape((T, F)),                                \n",
    "    layers.Conv1D(16, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "    layers.Conv1D(32, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "    layers.Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, name='y_pred', activation='softmax'),\n",
    "])\n",
    "model_inf.set_weights(model.get_weights())\n",
    "k2c(model_inf, \"mymodel\", num_tests=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ea93b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The following errors were found:\nLayer type 'GaussianNoise' is not supported at this time.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mk2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmymodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tests\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras2c\\keras2c_main.py:231\u001b[39m, in \u001b[36mk2c\u001b[39m\u001b[34m(model, function_name, malloc, num_tests, verbose)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    226\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mUnknown model type. Model should either be an instance of keras.Model, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    227\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mor a filepath to a saved .h5 model\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    228\u001b[39m     )\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Check that the model can be converted\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[43mcheck_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mAll checks passed\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras2c\\check_model.py:246\u001b[39m, in \u001b[36mcheck_model\u001b[39m\u001b[34m(model, function_name)\u001b[39m\n\u001b[32m    243\u001b[39m log += config_log\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (valid_fname \u001b[38;5;129;01mand\u001b[39;00m valid_lname \u001b[38;5;129;01mand\u001b[39;00m valid_layer \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    245\u001b[39m         valid_activation \u001b[38;5;129;01mand\u001b[39;00m valid_config):\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(log)\n",
      "\u001b[31mAssertionError\u001b[39m: The following errors were found:\nLayer type 'GaussianNoise' is not supported at this time.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0fca0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
